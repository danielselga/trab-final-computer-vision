{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T1_N1faWPXs"
      },
      "source": [
        "# MBA FIAP Inteligência Artificial & Machine Learning\n",
        "\n",
        "## Visão Computacional: Análise de Imagens Médicas\n",
        "\n",
        "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**.\n",
        "\n",
        "\n",
        "## 1. Introdução\n",
        "\n",
        "Uma determinada fintech focada em consumidores finais pessoa física constataou um grande número de fraudes em transações bancárias.\n",
        "\n",
        "O setor de fraudes apontou que existem clientes que se queixaram de não contratar serviços específicos, como o crédito pessoal, e após isso transferir para outras contas desconhecidas.\n",
        "\n",
        "Após análises pelas equipes de segurança, os protocolos de utilização da senha foram realizados em conformidade, ou seja, cada cliente autenticou com sua própria senha de maneira regular.\n",
        "\n",
        "Em função disso, o banco precisa arcar com reembolsos e medidas de contenção para evitar processos judiciais, pois os clientes alegam terem sido invadidos por hackers ou algo parecido.\n",
        "\n",
        "Uma das formas de solucionar ou minimizar este problema é com a utilização de outras formas de autenticação, sobretudo em operações críticas, como a obtenção de crédito pessoal.\n",
        "\n",
        "Desta forma podemos implementar uma verificação de identidade com prova de vida (liveness), que utilize uma verificação e identificação facial.\n",
        "\n",
        "Caso o cliente não seja autenticado, ele será atendido por uma esteira dedicada e as evidências da não identificação serão encaminhadas para a área de IA para validação dos parâmetros e limiares para aperfeiçoamento do modelo.\n",
        "\n",
        "Será necessário construir:\n",
        "\n",
        "* Detector de faces\n",
        "* Identificação de faces (podendo ser um comparador entre um rosto de documento e outra da prova de vida)\n",
        "* Detecção de vivacidade (liveness) para evitar que um fraudador utilize uma foto estática.\n",
        "\n",
        "\n",
        ">Formas alternativas de prover a identificação e prova de vivacidade, além destas que foram solicitadas poderão ser submetidas.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"imagens/liveness.jpg\">\n",
        "</p>\n",
        "\n",
        "Imagem retirada do [Grunge](https://www.grunge.com/192826/company-testing-robocop-facial-recognition-software-with-us-police/).\n",
        "\n",
        "## 2. Instruções\n",
        "\n",
        "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas.\n",
        "\n",
        "Iremos constuir uma forma de validar se uma determinada imagem foi ou não adulterada e se trata de uma produção fraudade.\n",
        "\n",
        "Existem diversas formas de validar a vivacidade, e neste sentido conto com a criatividade de vocês dado que já dominam encontrar uma face numa imagem, aplicar marcos faciais e até mesmo construir uma rede neural convulacional.\n",
        "\n",
        "A abordagem mais simples é pela construção de uma rede neural com imagens de fotos de rostos de outras fotos e fotos de rostos sem modificações. Tal classificador deverá classificar se dada imagem possui vivacidade ou não com uma pontuação de probabilidade.\n",
        "\n",
        "Referências que abordam o tema para servir de inspiração:\n",
        "\n",
        "1. [PyImageSearch](https://pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/), Liveness detection with OpenCV;\n",
        "2. [Kickertech](https://kickertech.com/face-liveness-detection-via-opencv-and-tensorflow/), Liveness detection via OpenCV and Tensorflow.\n",
        "3. [Towards Data Science](https://towardsdatascience.com/real-time-face-liveness-detection-with-python-keras-and-opencv-c35dc70dafd3?gi=24f8e1b740f9), Real-time face liveness detection with Python, Keras and OpenCV.\n",
        "\n",
        "Este projeto poderá ser feita por grupos de até 4 pessoas.\n",
        "Caso este projeto seja substitutivo, deverá ser realizado por apenas uma pessoa.\n",
        "\n",
        "| Nome dos Integrantes     | RM            | Turma |\n",
        "| :----------------------- | :------------- | :-----: |\n",
        "| Integrante 1             | RM 12345      | XIA |\n",
        "| Integrante 2             | RM 12345      | XIA |\n",
        "| Integrante 3             | RM 12345      | XIA |\n",
        "| Integrante 4             | RM 12345      | XIA |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC4lHvcxWPXt"
      },
      "source": [
        "## 3. Abordagem e organização da solução do problema (2 pontos)\n",
        "\n",
        "Como o grupo pretende deteccar a prova de vivacidade de uma determinada imagem? Quais os passos e os building blocks deste processo?\n",
        "\n",
        "\n",
        "Com base na sólida arquitetura do MobileNet, nossa equipe planeja incorporar técnicas avançadas de detecção de vivacidade em imagens, aprimorando ainda mais a capacidade do modelo. Inicialmente, conduziremos uma coleta abrangente e uma preparação minuciosa dos dados. Em seguida, procederemos ao treinamento do modelo, realizando ajustes estratégicos para garantir um desempenho otimizado.\n",
        "\n",
        "Após a fase de treinamento, realizaremos uma avaliação rigorosa do desempenho do modelo, empregando métricas específicas e analisando os resultados obtidos. Além disso, planejamos conduzir simulações extensivas para verificar a robustez e eficácia do modelo em diversas situações, proporcionando uma compreensão abrangente de sua capacidade de generalização.\n",
        "\n",
        "Ao longo desse processo, estaremos atentos a refinamentos contínuos, ajustes de parâmetros e a implementação de estratégias de otimização, visando alcançar uma detecção de vivacidade de alto desempenho e confiabilidade. Essa abordagem abrangente visa não apenas atender, mas também superar as expectativas em termos de qualidade e eficiência na detecção de vivacidade em imagens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JiolhSeWPXt"
      },
      "source": [
        "**Resposta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCzcoPAmWPXt"
      },
      "source": [
        "## 4 Desenvolvimento da solução (5,5 pontos)\n",
        "\n",
        "Detalhe o passo-a-passo do algoritmo de deteção de vivacidade.\n",
        "Se optar pela construção e treinamento de um modelo de redes neurais convulucionais, apresente a arquitetura, prepare os dados de treinamento, realize o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7ggXXVpWPXt"
      },
      "source": [
        "### 4.1 Organização de dados para treinamento de modelo de liveness (2 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "HDVJ4cY4WPXt",
        "outputId": "92c66abb-feda-41f5-e708-d47a55083f83"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8ce023561b8f>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_face_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_face_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_images_and_labels(urls, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                img = cv2.imdecode(np.asarray(bytearray(response.content)), 1)\n",
        "                if img is not None:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    img = cv2.resize(img, (224, 224))\n",
        "                    img = img.astype('float32') / 255.0\n",
        "                    images.append(img)\n",
        "                    labels.append(label)\n",
        "            else:\n",
        "                print(f\"Error downloading image from URL: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image from URL: {url}. Error: {e}\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "real_face_urls = [\n",
        "    \"https://raw.githubusercontent.com/danielselga/trab-final-computer-vision/main/WhatsApp%20Image%202024-02-07%20at%2010.37.37%20PM.jpeg\",\n",
        "]\n",
        "\n",
        "fake_face_urls = [\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "real_face_images, real_face_labels = load_images_and_labels(real_face_urls, 1)\n",
        "fake_face_images, fake_face_labels = load_images_and_labels(fake_face_urls, 0)\n",
        "\n",
        "X = np.concatenate((real_face_images, fake_face_images), axis=0)\n",
        "y = np.concatenate((real_face_labels, fake_face_labels), axis=0)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Number of training images:\", X_train.shape[0])\n",
        "print(\"Number of validation images:\", X_val.shape[0])\n",
        "print(\"Number of test images:\", X_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiZsODR9WPXt"
      },
      "source": [
        "### 4.2 Treinamento de modelo de liveness (1,5 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIJ4ho6fWPXu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, ReLU, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "\n",
        "def create_mobilenet(input_shape, num_classes):\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = DepthwiseConv2D(kernel_size=(3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(64, kernel_size=(1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    output_tensor = Dense(num_classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "    return model\n",
        "\n",
        "input_shape = (224, 224, 3)  # Input size of images\n",
        "num_classes = 1\n",
        "\n",
        "\n",
        "model = create_mobilenet(input_shape, num_classes)\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiPRBMg_WPXu"
      },
      "source": [
        "### 4.3 Métricas de desempenho do modelo (2 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0eXeowkWPXu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hkvwcaxWPXu"
      },
      "source": [
        "## 5 Teste Fim-a-Fim\n",
        "\n",
        "Simule a operação fim-a-fim, com uma imagem de entrada forjada (foto de foto de um rosto) e outra com uma imagem de rosto, exibindo o resultado da classificação e a pontuação de cada classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krg2KGWBWPXu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def show_result(image, title, score):\n",
        "    plt.imshow(image)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(f\"Score: {score:.2f}\")\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "forged_image_path = \"/content/WhatsApp Image 2024-01-29 at 19.37.29.jpeg\"\n",
        "real_image_path = \"/content/WhatsApp Image 2024-01-29 at 19.36.11.jpeg\"\n",
        "\n",
        "# Load images\n",
        "forged_image = cv2.imread(forged_image_path)\n",
        "real_image = cv2.imread(real_image_path)\n",
        "\n",
        "forged_image = cv2.resize(forged_image, (224, 224)) / 255.0\n",
        "real_image = cv2.resize(real_image, (224, 224)) / 255.0\n",
        "\n",
        "score_forged = model.predict(np.expand_dims(forged_image, axis=0))[0][0]\n",
        "score_real = model.predict(np.expand_dims(real_image, axis=0))[0][0]\n",
        "\n",
        "show_result(forged_image, \"Forged Image\", score_forged)\n",
        "show_result(real_image, \"Real Image\", score_real)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7anGQxNVWPXu"
      },
      "source": [
        ">Com a implementação da solução na forma de uma aplicação do [Streamlit](https://www.streamlit.io/) (veja a pata streamlit-app e use o template) vale 1 ponto adicional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trrcRbA_WPXu"
      },
      "source": [
        "**Pergunta**: Se utilizou o Streamlit, compartilhe a URL do aplicativo publicado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBffBwAOWPXu"
      },
      "source": [
        "**Resposta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHiJ1eoTWPXv"
      },
      "source": [
        "## 6 Conclusões (2,5 pontos)\n",
        "\n",
        "**Pergunta**: Dado todo o estudo e pesquisa, quais foram as conclusões sobre a solução, o que funcionou, o que não funcionou e quais os detalhes que observariam numa nova versão e melhorias do processo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefRNH_5WPXv"
      },
      "source": [
        "**Resposta**:"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "733a071da2455ea0e8bdf5409a7097e630ac701195faf55c6e985d77ee3ec176"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}